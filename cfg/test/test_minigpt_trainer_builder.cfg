[model::minigpt_articles-small]
embd_pdrop = 0.1
resid_pdrop = 0.1
attn_pdrop = 0.1
n_layer = 8
n_head = 8
n_embd = 512
; vocab_size
; ... derived from tokenizer
vocab_size = dataset::articles-mini-part-64/layout/tokenizer/vocab_size@articles_mini_part
; block_size aka max_seq_len
; ... derived from layout
block_size = dataset::articles-mini-part-64/layout/window_size@articles_mini_part

[trainer::minigpt_articles-mini-part-64]
fs = fs::articles@articles
dataset = dataset::articles-mini-part-64@articles_mini_part
model = model::minigpt_articles-small
;
max_epochs = 10
batch_size = 64
learning_rate = 3e-4
betas = (0.9, 0.95)
grad_norm_clip = 1.0
; # only applied on matmul weights
weight_decay = 0.1
; learning rate decay params:
; ... linear warmup followed by cosine decay to 10% of original
lr_decay = false
; these two numbers come from the GPT-3 paper,
; ... but may not be good defaults elsewhere
warmup_tokens = 375e6
; ... at what point we reach 10% of original LR
final_tokens = 260e9

