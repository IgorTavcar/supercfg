[fs::articles]
root = ../data
domain = 'articles'

[importer::articles-no_sections]
source = ../data/corpus/slovenian/part1/raw_text_only_digital_text_chunks
source_pattern = pattern:^chunk\.[a-z]{3}$
destination_fs = fs::articles
destination_ds = articles-no_sections
process = process.IDXArticleImporterProcess
article_sections = false

[tokenizer::gpt-bpe-30_000]
fs = fs::articles
target_model = gpt
vocab_size = 30_000
add_prefix_space = true

[layout::512-0.5]
tokenizer = tokenizer::gpt-bpe-30_000
process = 'process.Aligner'
window_size = 512
window_step = 0.5
alignment = space
adaptive = true
edge_tokens = (<|eot|>, <|eot|>, false)

[layout::64-0.5]
tokenizer = tokenizer::gpt-bpe-30_000
process = 'process.Aligner'
window_size = 64
window_step = 0.5
alignment = space
adaptive = true
edge_tokens = (<|eot|>, <|eot|>, false)

[layout::512-0.5-edged]
tokenizer = tokenizer::gpt-bpe-30_000
process = 'process.Aligner'
window_size = 512
window_step = 0.5
alignment = space
adaptive = true
edge_tokens = (<|eot|>, <|eot|>, true)

[dataset::articles-64]
fs = fs::articles
importer = importer::articles-no_sections
layout = layout::64-0.5
tag = "64-0.5-0.1"
target_tasks = (causal_language_modeling, masked_language_modeling)
parts = (train=0.9, test=0.1)

[dataset::articles-512]
fs = fs::articles
importer = importer::articles-no_sections
layout = layout::512-0.5
tag = "512-0.5-0.1"
target_tasks = (causal_language_modeling, masked_language_modeling)
parts = (train=0.9, test=0.1)

[dataset::articles-512-edged]
fs = fs::articles
importer = importer::articles-no_sections
layout = layout::512-0.5-edged
tag = "512-0.5-0.1-edged"
target_tasks = (causal_language_modeling, masked_language_modeling)
parts = (train=0.9, test=0.1)

[model::gpt-articles-512]
type=gpt
dataset=dataset::articles-512
hidden_size=1024
intermediate_size=1024
num_layers=8
num_heads=8
eos_token='<|eos|>'
