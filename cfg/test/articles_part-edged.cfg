[fs::articles]
root = ../data
domain = 'articles'

[importer::articles_part-no_sections]
source = ../data/corpus/slovenian/part1/raw_text_only_digital_text_chunks_part
source_pattern = pattern:^chunk\.[a-z]{3}$
destination_fs = fs::articles
destination_ds = articles_part-no_sections
process = process.IDXArticleImporterProcess
article_sections = false

[tokenizer::gpt-bpe-30_000]
fs = fs::articles
target_model = gpt
vocab_size = 30_000
add_prefix_space = true

[layout::512-0.5-edged]
tokenizer = tokenizer::gpt-bpe-30_000
process = 'process.Aligner'
window_size = 512
window_step = 0.5
alignment = space
adaptive = true
edge_tokens = (<|eot|>, <|eot|>, true)

[dataset::articles_part-edged]
fs = fs::articles
importer = importer::articles_part-no_sections
layout = layout::512-0.5-edged
tag = "512-0.5"
target_tasks = (causal_language_modeling, masked_language_modeling)
parts = (train=0.9, test=0.1)


